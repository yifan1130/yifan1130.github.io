---
---

@inproceedings{10.1145/3731569.3764829,
author = {Yu, Yifan* and Gan, Yu* and Sarda, Nikhil and Tsai, Lillian and Shen, Jiaming and Zhou, Yanqi and Krishnamurthy, Arvind and Lai, Fan and Levy, Hank and Culler, David},
title = {IC-Cache: Efficient Large Language Model Serving via In-context Caching},
year = {2025},
isbn = {9798400718700},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731569.3764829},
doi = {10.1145/3731569.3764829},
abstract = {Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 70\% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge transfer among requests. However, naively caching and reusing past responses leads to a big quality drop.In this paper, we introduce IC-Cache, a caching system that enables live LLM capability augmentation to improve serving efficiency: by leveraging historical request-response pairs from larger models as in-context examples, IC-Cache empowers small LLMs to imitate and even exceed the compositional abilities (e.g., reasoning) of their larger counterparts, enabling selective offloading of requests to reduce cost and latency. Achieving this live augmentation at scale introduces intricate trade-offs between response quality, latency, and system throughput. For a new request, IC-Cache efficiently selects similar, high-utility examples to prepend them to the new request's input. At scale, it adaptively routes requests across LLMs of varying capabilities, accounting for response quality and serving loads. IC-Cache employs a cost-aware cache replay mechanism that refines example quality offline to maximize online cache utility and efficiency. Evaluations on millions of realistic requests demonstrate that IC-Cache improves LLM serving throughput by 1.4–5.9x and reduces latency by 28–71\% without hurting response quality.},
booktitle = {Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles},
pages = {375-398},
numpages = {24},
keywords = {large language models (LLMs), LLM serving, cloud computing, semantic caching, request routing, load balancing, quality-efficiency tradeoff},
location = {Lotte Hotel World, Seoul, Republic of Korea},
series = {SOSP '25},
preview={echolm.png},
award_name={31st SOSP},
award={Acceptance rate: 17 percent},
selected={true},
annotation={* Example Contribution},
}

@article{li2023loftq,
  title={Loftq: Lora-fine-tuning-aware quantization for large language models},
  author={Li, Yixiao* and Yu, Yifan* and Liang, Chen and He, Pengcheng and Karampatziakis, Nikos and Chen, Weizhu and Zhao, Tuo},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024},
  preview={loftq.png},
  selected={true},
  annotation={* Example Contribution},
  award={About 1 percent of the submissions are selected as oral presentation},
  award_name={Oral Presentation}
}

@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao* and Yu, Yifan* and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR},
  annotation={* Example Contribution},
  preview={losparse.png},
  selected={true},
}

